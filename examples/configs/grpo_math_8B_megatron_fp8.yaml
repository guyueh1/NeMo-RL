# GRPO Algorithm Configuration
defaults: "grpo_math_8B_megatron.yaml"

loss_fn:
  use_importance_sampling_correction: true

policy:
  generation:
    vllm_cfg:
      precision: 'fp8'
      use_deep_gemm: true
      gpu_memory_utilization: 0.5
    vllm_kwargs:
      compilation_config:
        pass_config:
          enable_fusion: true
          enable_noop: true
  megatron_cfg:
    tensor_model_parallel_size: 2
    fp8_cfg:
      enabled: true
      fp8: "hybrid"
      fp8_recipe: "blockwise"
      fp8_param: true
    optimizer:
      use_precision_aware_optimizer: false
    distributed_data_parallel_config:
      grad_reduce_in_fp32: true
logger:
  wandb_enabled: false
  wandb:
    project: "nemo-rl-grpo-dev-guyueh"
    name: "grpo_math_8B_megatron_fp8_debug"
checkpointing:
  enabled: false