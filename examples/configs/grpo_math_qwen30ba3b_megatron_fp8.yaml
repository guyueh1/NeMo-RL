# GRPO Algorithm Configuration
defaults: "grpo_math_qwen30ba3b_megatron.yaml"

loss_fn:
  use_importance_sampling_correction: true

policy:
  megatron_cfg:
    moe_router_dtype: fp32
    
    fp8_cfg:
      enabled: true
      fp8: "e4m3"
      fp8_recipe: "blockwise"
      fp8_param: false

    optimizer:
      use_precision_aware_optimizer: false
    
    env_vars:
      NVTE_FP8_BLOCK_SCALING_FP32_SCALES: "1"

  generation:
    vllm_cfg:
      tensor_parallel_size: 8
      expert_parallel_size: 8 # need to make moe_intermediate_size / expert_tensor_parallel_size % 128 == 0
      precision: "fp8"
      use_deep_gemm: true
      gpu_memory_utilization: 0.5